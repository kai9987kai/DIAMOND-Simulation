<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>DIAMOND Simulation with Q-Learning and Neural Network Agents</title>
    <style>
        /* Styles for the simulation */
        body {
            font-family: Arial, sans-serif;
            background-color: #fafafa;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 1400px;
            margin: auto;
        }
        h1 {
            color: #222;
            text-align: center;
        }
        .canvas-container {
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
        }
        canvas {
            border: 1px solid #999;
            background-color: #fff;
            margin: 10px;
        }
        .controls {
            margin-top: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            align-items: center;
        }
        .controls label {
            margin-right: 5px;
        }
        .info-panel {
            margin-top: 20px;
            background-color: #eaeaea;
            padding: 15px;
            border-radius: 8px;
        }
        .agent-info {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }
        .agent-info div {
            flex: 1;
            min-width: 150px;
        }
        .slider-label {
            display: flex;
            align-items: center;
        }
        .slider-label input {
            margin-left: 5px;
        }
        .data-visualization {
            margin-top: 20px;
        }
        .chart-container {
            width: 100%;
            height: 300px;
            margin-bottom: 30px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Hybrid DIAMOND Simulation with Q-Learning & Neural Networks</h1>
        <div class="canvas-container">
            <div>
                <h2>World State</h2>
                <canvas id="world-canvas" width="600" height="600"></canvas>
            </div>
            <div>
                <h2>Agent's Prediction</h2>
                <canvas id="prediction-canvas" width="600" height="600"></canvas>
            </div>
        </div>
        <div class="controls">
            <button id="reset-button">Reset</button>
            <button id="step-button">Step</button>
            <button id="run-button">Run</button>
            <label for="diffusion-steps">Diffusion Steps:</label>
            <input type="number" id="diffusion-steps" value="1" min="1" max="3">
            <label for="learning-rate">Learning Rate (α):</label>
            <input type="number" id="learning-rate" value="0.1" min="0.01" max="1" step="0.01">
            <label for="discount-factor">Discount Factor (γ):</label>
            <input type="number" id="discount-factor" value="0.9" min="0.1" max="1" step="0.1">
            <label for="epsilon">Exploration Rate (ε):</label>
            <input type="number" id="epsilon" value="0.1" min="0" max="1" step="0.05">
            <div class="slider-label">
                <label for="agent-speed">Agent Speed:</label>
                <input type="range" id="agent-speed" value="5" min="1" max="10">
            </div>
            <label for="num-agents">Number of Agents:</label>
            <input type="number" id="num-agents" value="1" min="1" max="5">
        </div>
        <div class="info-panel">
            <div class="agent-info" id="agent-info">
                <!-- Agent information will be populated here -->
            </div>
            <p>Total Steps: <span id="total-steps">0</span></p>
            <p>Average Prediction Accuracy: <span id="prediction-accuracy">0%</span></p>
        </div>
        <div class="data-visualization">
            <h2>Agent Performance Metrics</h2>
            <canvas id="reward-chart" class="chart-container"></canvas>
            <canvas id="energy-chart" class="chart-container"></canvas>
        </div>
    </div>

    <!-- Include TensorFlow.js and Chart.js libraries before the simulation script -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js"></script>
    <!-- Simulation script -->
    <script>
        // Configuration Constants
        const GRID_SIZE = 60;
        const CELL_SIZE = 10;
        const CANVAS_SIZE = GRID_SIZE * CELL_SIZE;
        const MAX_ENERGY = 200;

        // DOM elements
        const worldCanvas = document.getElementById('world-canvas');
        const predictionCanvas = document.getElementById('prediction-canvas');
        const worldCtx = worldCanvas.getContext('2d');
        const predictionCtx = predictionCanvas.getContext('2d');

        const resetButton = document.getElementById('reset-button');
        const stepButton = document.getElementById('step-button');
        const runButton = document.getElementById('run-button');
        const diffusionStepsInput = document.getElementById('diffusion-steps');
        const learningRateInput = document.getElementById('learning-rate');
        const discountFactorInput = document.getElementById('discount-factor');
        const epsilonInput = document.getElementById('epsilon');
        const agentSpeedInput = document.getElementById('agent-speed');
        const numAgentsInput = document.getElementById('num-agents');

        const agentInfoContainer = document.getElementById('agent-info');
        const totalStepsDisplay = document.getElementById('total-steps');
        const predictionAccuracyDisplay = document.getElementById('prediction-accuracy');

        const rewardChartCtx = document.getElementById('reward-chart').getContext('2d');
        const energyChartCtx = document.getElementById('energy-chart').getContext('2d');

        // Hybrid Agent class (combining Q-Learning and Neural Network)
        class HybridAgent {
            constructor(environment, id) {
                this.id = id;
                this.env = environment;
                this.position = [CANVAS_SIZE / 2, CANVAS_SIZE / 2];
                this.qTable = {};  // Q-Learning Q-Table
                this.initializeQTable();
                this.model = null;  // Neural network for higher-level decisions
                this.initializeNeuralNetwork();
                this.alpha = parseFloat(learningRateInput.value);  // Learning rate
                this.gamma = parseFloat(discountFactorInput.value);  // Discount factor
                this.epsilon = parseFloat(epsilonInput.value);  // Exploration rate
                this.totalReward = 0;
                this.totalSteps = 0;
                this.energy = MAX_ENERGY;
                this.path = [];
            }

            // Create Q-Table
            initializeQTable() {
                for (let y = 0; y < this.env.gridSize; y++) {
                    for (let x = 0; x < this.env.gridSize; x++) {
                        const state = `${x},${y}`;
                        this.qTable[state] = { 'up': 0, 'down': 0, 'left': 0, 'right': 0 };
                    }
                }
            }

            // Create a neural network using TensorFlow.js with error handling
            initializeNeuralNetwork() {
                try {
                    this.model = tf.sequential();
                    this.model.add(tf.layers.dense({ units: 64, inputShape: [2], activation: 'relu' }));
                    this.model.add(tf.layers.dense({ units: 4, activation: 'linear' }));
                    this.model.compile({ optimizer: tf.train.adam(), loss: 'meanSquaredError' });
                } catch (error) {
                    console.error('Error initializing TensorFlow model:', error);
                    alert('TensorFlow failed to load. Neural network functionality will be disabled.');
                    this.model = null;  // Fallback if TensorFlow is not available
                }
            }

            // Predict next action using the neural network
            async predictAction(state) {
                if (!this.model) {
                    return [0, 0, 0, 0];  // Fallback if model is not available
                }
                try {
                    const input = tf.tensor2d([state], [1, 2]);
                    const prediction = await this.model.predict(input).data();
                    return prediction;
                } catch (error) {
                    console.error('Prediction error:', error);
                    return [0, 0, 0, 0];  // Fallback if prediction fails
                }
            }

            // Combine Q-learning and Neural Network for action choice
            async chooseAction() {
                const currentGridX = Math.floor(this.position[0] / CELL_SIZE);
                const currentGridY = Math.floor(this.position[1] / CELL_SIZE);
                const state = [currentGridX / GRID_SIZE, currentGridY / GRID_SIZE];

                // Use Q-learning for short-term decisions
                const qAction = this.qLearningChooseAction(currentGridX, currentGridY);

                // Use neural network for higher-level strategies
                const nnPredictions = await this.predictAction(state);
                const actions = ['up', 'down', 'left', 'right'];
                const nnAction = actions[nnPredictions.indexOf(Math.max(...nnPredictions))];

                // Hybrid approach: sometimes use NN, sometimes use Q-learning based on energy level
                return this.energy < MAX_ENERGY * 0.5 ? qAction : nnAction;
            }

            // Q-learning decision
            qLearningChooseAction(x, y) {
                const state = `${x},${y}`;
                if (Math.random() < this.epsilon) {
                    const actions = Object.keys(this.qTable[state]);
                    return actions[Math.floor(Math.random() * actions.length)];
                } else {
                    const actions = this.qTable[state];
                    return Object.keys(actions).reduce((a, b) => actions[a] > actions[b] ? a : b);
                }
            }

            // Perform agent movement and learning (Q-learning & NN combined)
            async move() {
                const currentGridX = Math.floor(this.position[0] / CELL_SIZE);
                const currentGridY = Math.floor(this.position[1] / CELL_SIZE);
                const currentState = [currentGridX / GRID_SIZE, currentGridY / GRID_SIZE];

                const action = await this.chooseAction();
                let [dx, dy] = [0, 0];

                switch (action) {
                    case 'up': dy = -1; break;
                    case 'down': dy = 1; break;
                    case 'left': dx = -1; break;
                    case 'right': dx = 1; break;
                }

                let newX = currentGridX + dx;
                let newY = currentGridY + dy;
                newX = Math.max(0, Math.min(this.env.gridSize - 1, newX));
                newY = Math.max(0, Math.min(this.env.gridSize - 1, newY));

                const reward = this.env.state[newY][newX];
                const newState = [newX / GRID_SIZE, newY / GRID_SIZE];

                this.updateQTable(currentState, action, reward, newState);

                this.trainNeuralNetwork(currentState, action, reward, newState);

                this.position = [newX * CELL_SIZE + CELL_SIZE / 2, newY * CELL_SIZE + CELL_SIZE / 2];
                this.path.push([...this.position]);
                if (this.path.length > 100) this.path.shift();

                this.totalReward += reward;
                this.energy -= 0.5;  // Energy consumption
                this.totalSteps++;
            }

            // Q-learning update
            updateQTable(currentState, action, reward, nextState) {
                const currentQ = this.qTable[`${Math.floor(currentState[0] * GRID_SIZE)},${Math.floor(currentState[1] * GRID_SIZE)}`][action];
                const nextMaxQ = Math.max(...Object.values(this.qTable[`${Math.floor(nextState[0] * GRID_SIZE)},${Math.floor(nextState[1] * GRID_SIZE)}`]));
                this.qTable[`${Math.floor(currentState[0] * GRID_SIZE)},${Math.floor(currentState[1] * GRID_SIZE)}`][action] +=
                    this.alpha * (reward + this.gamma * nextMaxQ - currentQ);
            }

            // Train neural network using backpropagation
            async trainNeuralNetwork(currentState, action, reward, nextState) {
                if (!this.model) return;  // Skip training if model isn't available
                try {
                    const target = await this.predictAction(currentState);
                    const actions = ['up', 'down', 'left', 'right'];
                    target[actions.indexOf(action)] = reward;

                    const input = tf.tensor2d([currentState], [1, 2]);
                    const targetTensor = tf.tensor2d([target], [1, 4]);

                    await this.model.fit(input, targetTensor);
                } catch (error) {
                    console.error('Training error:', error);
                }
            }
        }

        // Environment Class
        class Environment {
            constructor(gridSize) {
                this.gridSize = gridSize;
                this.state = this.createState(gridSize);
            }

            // Create environment state with random rewards
            createState(gridSize) {
                let state = [];
                for (let y = 0; y < gridSize; y++) {
                    let row = [];
                    for (let x = 0; x < gridSize; x++) {
                        row.push(Math.random() > 0.8 ? 10 : -1);  // Random rewards or penalties
                    }
                    state.push(row);
                }
                return state;
            }
        }

        // Simulation Class
        class Simulation {
            constructor() {
                this.env = new Environment(GRID_SIZE);
                this.agents = [];
                this.isRunning = false;
                this.totalSteps = 0;
                this.dataLog = [];
                this.rewardChart = null;
                this.energyChart = null;
                this.initializeAgents();
                this.initializeCharts();
                this.updateDisplay();
                this.draw();

                this.lastFrameTime = performance.now();
                this.targetFPS = 30;
                this.frameInterval = 1000 / this.targetFPS;
            }

            initializeAgents() {
                const numAgents = parseInt(numAgentsInput.value);
                this.agents = [];
                for (let i = 0; i < numAgents; i++) {
                    this.agents.push(new HybridAgent(this.env, i + 1));
                }
                this.updateAgentInfoDisplay();
            }

            initializeCharts() {
                // Initialize reward and energy charts using Chart.js
                if (this.rewardChart) this.rewardChart.destroy();
                if (this.energyChart) this.energyChart.destroy();

                this.rewardChart = new Chart(rewardChartCtx, {
                    type: 'line',
                    data: {
                        labels: [],
                        datasets: this.agents.map(agent => ({
                            label: `Agent ${agent.id} Reward`,
                            data: [],
                            borderColor: `hsl(${agent.id * 60}, 100%, 50%)`,
                            fill: false
                        }))
                    },
                    options: {
                        responsive: true,
                        scales: {
                            x: { title: { display: true, text: 'Steps' } },
                            y: { title: { display: true, text: 'Reward' } }
                        }
                    }
                });

                this.energyChart = new Chart(energyChartCtx, {
                    type: 'line',
                    data: {
                        labels: [],
                        datasets: this.agents.map(agent => ({
                            label: `Agent ${agent.id} Energy`,
                            data: [],
                            borderColor: `hsl(${agent.id * 60 + 180}, 100%, 50%)`,
                            fill: false
                        }))
                    },
                    options: {
                        responsive: true,
                        scales: {
                            x: { title: { display: true, text: 'Steps' } },
                            y: { title: { display: true, text: 'Energy' } }
                        }
                    }
                });
            }

            async step() {
                this.totalSteps++;
                for (let agent of this.agents) {
                    await agent.move();
                    this.dataLog.push({
                        step: this.totalSteps,
                        agentId: agent.id,
                        reward: agent.totalReward,
                        energy: agent.energy
                    });
                }
                this.updateCharts();
                this.updateDisplay();
                this.draw();
            }

            start() {
                this.isRunning = true;
                this.runLoop();
            }

            stop() {
                this.isRunning = false;
            }

            reset() {
                this.env = new Environment(GRID_SIZE);
                this.initializeAgents();
                this.totalSteps = 0;
                this.dataLog = [];
                this.initializeCharts();
                this.updateDisplay();
                this.draw();
            }

            async runLoop() {
                while (this.isRunning) {
                    const currentTime = performance.now();
                    const deltaTime = currentTime - this.lastFrameTime;
                    if (deltaTime >= this.frameInterval) {
                        this.lastFrameTime = currentTime;
                        await this.step();
                    }
                    await new Promise(resolve => setTimeout(resolve, 10));  // Adding a small delay to prevent freezing
                }
            }

            updateCharts() {
                if (!this.rewardChart || !this.energyChart) return;

                // Update reward chart
                this.rewardChart.data.labels.push(this.totalSteps);
                for (let i = 0; i < this.agents.length; i++) {
                    this.rewardChart.data.datasets[i].data.push(this.agents[i].totalReward);
                }
                this.rewardChart.update();

                // Update energy chart
                this.energyChart.data.labels.push(this.totalSteps);
                for (let i = 0; i < this.agents.length; i++) {
                    this.energyChart.data.datasets[i].data.push(this.agents[i].energy);
                }
                this.energyChart.update();
            }

            updateDisplay() {
                totalStepsDisplay.textContent = this.totalSteps;
                let totalAccuracy = 0;
                for (let agent of this.agents) {
                    totalAccuracy += agent.totalReward;
                }
                predictionAccuracyDisplay.textContent = `${((totalAccuracy / (this.totalSteps * this.agents.length)) * 100).toFixed(2)}%`;
                this.updateAgentInfoDisplay();
            }

            updateAgentInfoDisplay() {
                agentInfoContainer.innerHTML = '';
                for (let agent of this.agents) {
                    const infoDiv = document.createElement('div');
                    infoDiv.textContent = `Agent ${agent.id}: Reward = ${agent.totalReward}, Energy = ${agent.energy}`;
                    agentInfoContainer.appendChild(infoDiv);
                }
            }

            draw() {
                worldCtx.clearRect(0, 0, worldCanvas.width, worldCanvas.height);
                predictionCtx.clearRect(0, 0, predictionCanvas.width, predictionCanvas.height);

                // Draw environment
                for (let y = 0; y < this.env.gridSize; y++) {
                    for (let x = 0; x < this.env.gridSize; x++) {
                        const value = this.env.state[y][x];
                        if (value > 0) {
                            worldCtx.fillStyle = 'green';
                        } else {
                            worldCtx.fillStyle = 'red';
                        }
                        worldCtx.fillRect(x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                    }
                }

                // Draw agents
                for (let agent of this.agents) {
                    worldCtx.fillStyle = 'blue';
                    worldCtx.beginPath();
                    worldCtx.arc(agent.position[0], agent.position[1], CELL_SIZE / 2, 0, 2 * Math.PI);
                    worldCtx.fill();
                }

                // Draw prediction (for demonstration purposes, simply mirroring world state)
                for (let y = 0; y < this.env.gridSize; y++) {
                    for (let x = 0; x < this.env.gridSize; x++) {
                        const value = this.env.state[y][x];
                        if (value > 0) {
                            predictionCtx.fillStyle = 'green';
                        } else {
                            predictionCtx.fillStyle = 'red';
                        }
                        predictionCtx.fillRect(x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE);
                    }
                }

                // Draw agent paths
                for (let agent of this.agents) {
                    predictionCtx.strokeStyle = 'blue';
                    predictionCtx.beginPath();
                    for (let i = 0; i < agent.path.length; i++) {
                        const [x, y] = agent.path[i];
                        if (i === 0) {
                            predictionCtx.moveTo(x, y);
                        } else {
                            predictionCtx.lineTo(x, y);
                        }
                    }
                    predictionCtx.stroke();
                }
            }
        }

        let simulation = new Simulation();

        resetButton.addEventListener('click', () => {
            simulation.reset();
            runButton.textContent = 'Run';
        });
        stepButton.addEventListener('click', () => simulation.step());
        runButton.addEventListener('click', () => {
            if (simulation.isRunning) {
                simulation.stop();
                runButton.textContent = 'Run';
            } else {
                simulation.start();
                runButton.textContent = 'Stop';
            }
        });

        numAgentsInput.addEventListener('change', () => {
            simulation.initializeAgents();
            simulation.initializeCharts();
        });
    </script>
</body>
</html>
